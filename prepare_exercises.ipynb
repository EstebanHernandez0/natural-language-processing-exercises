{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "344a07df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re as r\n",
    "import unicodedata\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ccc1aa",
   "metadata": {},
   "source": [
    "## Define a function named basic_clean. It should take in a string and apply some basic text cleaning to it:\n",
    " - Lowercase everything\n",
    " - Normalize unicode characters\n",
    " - Replace anything that is not a letter, number, whitespace or a single quote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "85cde90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_clean(x):\n",
    "    x= x.lower()\n",
    "    x= unicodedata.normalize('NFKD', x).encode('ascii', 'ignore').decode('utf-8')\n",
    "    x= r.sub(r'[^a-z0-9\\s]', '', x)\n",
    "    print(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "78627fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello my name is pdgetshawn\n"
     ]
    }
   ],
   "source": [
    "x= \"Hello, my name is pd.get_shawn.\"\n",
    "\n",
    "x= basic_clean(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa89675",
   "metadata": {},
   "source": [
    "## Define a function named tokenize. It should take in a string and tokenize all the words in the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1fa3f7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(x):\n",
    "    tokenize= nltk.tokenize.ToktokTokenizer()\n",
    "    x= tokenize.tokenize(x)\n",
    "    print(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1ff1fdb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'my', 'name', 'is', 'pdgetshawn']\n"
     ]
    }
   ],
   "source": [
    "x= tokenize(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d10b0ae",
   "metadata": {},
   "source": [
    "## Define a function named stem. It should accept some text and return the text after applying stemming to all the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "908ba2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(x):\n",
    "    ps= nltk.porter.PorterStemmer()\n",
    "    x= [ps.stem(word) for word in x]\n",
    "    ' '.join(x)\n",
    "    return x\n",
    "\n",
    "ps= nltk.porter.PorterStemmer()\n",
    "x= [ps.stem(word) for word in x]\n",
    "' '.join(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "63292f8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'my', 'name', 'is', 'pdgetshawn']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stems= stem(x)\n",
    "stems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6b6f20",
   "metadata": {},
   "source": [
    "## Define a function named lemmatize. It should accept some text and return the text after applying lemmatization to each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "372bbc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(x):\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    lemmas = [wnl.lemmatize(word) for word in x]\n",
    "    ' '.join(lemmas)\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adc02ae",
   "metadata": {},
   "source": [
    "## Define a function named remove_stopwords. It should accept some text and return the text after removing all the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f13e9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "66d3f3da",
   "metadata": {},
   "source": [
    "## This function should define two optional parameters, extra_words and exclude_words. These parameters should define any additional stop words to include, and any words that we don't want to remove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18dc059d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "73e8b246",
   "metadata": {},
   "source": [
    "## Use your data from the acquire to produce a dataframe of the news articles. Name the dataframe news_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf877d38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "24325372",
   "metadata": {},
   "source": [
    "## Make another dataframe for the Codeup blog posts. Name the dataframe codeup_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38dcaff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
